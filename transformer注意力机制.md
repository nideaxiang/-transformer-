# 一. 从函数到神经网络

- 符号主义：早期的人工智能认为precise的function 可以表示一切
- 联结主义：逼近就行
- 激活函数：将线性函数得到的结果进行非线性化
  - 线性函数套上非线性函数理论上可以逼近任意连续函数

### 如下图 函数可以与神经网络进行类比，就是线性函数套上一个激活函数就可以是一个神经元，那么 “神经网络的本质就是线性变换”

 

![image-20250828173501802](E:/markdown_pic/assets/image-20250828173501802.png)

从神经网络的角度，从左到右完成了计算，所以，这个过程叫做神经网络的前向传播

![image-20250828173704352](E:/markdown_pic/assets/image-20250828173704352.png)

#### 所以，最终不论多复杂，我们的目的只有一个，就是获得函数的参数，也就是上图中的w和b



# 二 从损失函数到梯度下降

## 1.损失函数：

我们需要求一个拟合效果很好的 w 和 b，也就是预测值和真实值之间的差值要尽可能地小一些，总的差值的反映可以是损失函数

最简单地话就是直接用绝对值：

![image-20250831155853515](E:/markdown_pic/assets/image-20250831155853515.png)

也可以用MSN

![image-20250831155943785](E:/markdown_pic/assets/image-20250831155943785.png)

### 所以损失函数用来找W B的过程就可以简化为：求极值点的过程

![image-20250831160055421](E:/markdown_pic/assets/image-20250831160055421.png)

假如考虑两个变量，也就是w和b，那么损失函数的图像就是一个三维图像

![image-20250831160520093](E:/markdown_pic/assets/image-20250831160520093.png)

这个时候的目标就是让每个参数的偏导数为0

![image-20250831160556198](E:/markdown_pic/assets/image-20250831160556198.png)

### 以上都是线性问题下的分析，也就是最简单的线性回归



## 2.梯度下降：对于非线性的问题，一点点试

### 基本思想：每次调整参数，使得参数向损失函数变小的方向调整

### 数学逻辑：某个参数变化一点点，使得损失函数变化多少，其实就是损失函数对某参数的偏导数，也就是使得w 和 b向着偏导数的反方向去变化。

### 梯度：所有参数的偏导数构成的向量就是梯度

### 梯度下降：不断变化参数，使得损失函数不断减小的过程



## 3.链式法则：复杂的神经网络如何求得偏导数进行梯度下降

例子是：有一个三层的神经网络，其计算逻辑如下，一共有四个参数需要求解

![image-20250831164142773](E:/markdown_pic/assets/image-20250831164142773.png)

那么就看w1变化一个单位会使得a变化多少，a变化一个单位又会使得yhat变化多少,如此类推看最终损失函数L的变化，最终的式子就是：

![image-20250831164418072](E:/markdown_pic/assets/image-20250831164418072.png)

## 4.反向传播：从右到左计算参数梯度

#### 这里可以发现，我们在计算损失函数对于各个参数的偏导数时候，优先计算靠近损失函数的参数，也就是靠输出端的参数，可以计算更少的参数，且从右向左计算，可以利用之前计算过的的偏导数，形象地说，就是从左到右传播过去

![image-20250831170208017](E:/markdown_pic/assets/image-20250831170208017.png)

一次训练的过程就是先进行前行传播，计算出yhat,再进行反向传播计算出梯度进行下降：

![image-20250831170527520](E:/markdown_pic/assets/image-20250831170527520.png)



# 三 训练神经网络的问题与解决方案

## 1. 过拟合：在训练数据上表现得很好，但在未知数据上很差

如下图，过拟合表示泛化能力差

![image-20250831171030549](E:/markdown_pic/assets/image-20250831171030549.png)

## 2.为什么会过拟合：模型太复杂，将数据集中的噪声和随机波动也给学习了进去

#### 解决： 

#### way1——简化模型的复杂度  

#### way2——增加训练数据的量

#### way3——数据增强： 除了泛化性和能增加模型的鲁棒性

#### 本身无法收集更多数据—>那么可以在原有数据中创造更多的数据：

- 如图像数据中进行旋转、裁剪和夹噪声等操作

![image-20250831172500830](E:/markdown_pic/assets/image-20250831172500830.png)

#### way4——提前终止训练（从训练过程入手，避免参数过度下降），但这种方式过于简单

#### way5——正则化：抑制参数的野蛮变化

在损失函数中加上参数本身的值形成新损失函数 **新损失函数=损失函数+各参数累加值**（惩罚项）

这样的话，在增加参数本身的值（绝对值/平方 ）时候，并没有使得原来的损失函数L减少，反而使得新损失函数减少，那么这个时候的就不应该进行这个调整



L2正则化因为是平方会使得抑制效果更明显

![image-20250831182639882](E:/markdown_pic/assets/image-20250831182639882.png)

弄清楚的概念：

- 惩罚项 
- 超参数
- 正则化系数
- L1正则化：绝对值之和叫做L1范数  
- L2正则化：平方和的平方根叫做L2范数  



#### way5——drop out:防止模型过于依赖少数某几个参数

每次训练随机丢掉几个参数，使得模型不总是依赖与少数参数，学会依赖更多的参数



## 3.梯度消失：网络越深，梯度反向传播越来越小 

解决：残差网络

## 4. 梯度爆炸：梯度数值越来越大，参数的调整幅度失去了控制

解决：梯度裁剪

## 5.收敛过慢：陷入局部最优/来回震荡





# 四.从网络到CNN的卷积运算

### 1.可以将复杂的非线性神经网络表示写为矩阵

如下图三个输入，两个输出的情况

![image-20250831202813799](E:/markdown_pic/assets/image-20250831202813799.png)

可以数学表达为：

![image-20250831202625591](E:/markdown_pic/assets/image-20250831202625591.png)

**Y=g(W·X+b)**

### 2.简化多层表示：利用a的上标，从0开始

![image-20250831203117457](E:/markdown_pic/assets/image-20250831203117457.png)

通用的公式就是：每一层神经元的输出值，都是关于上一层的函数

![image-20250831203139102](E:/markdown_pic/assets/image-20250831203139102.png)

### 3.全连接层FC的局限性：将会造成参数值的爆炸——>解决 卷积运算

### 4.卷积运算：

- 卷积核：移动的固定的矩阵，其值是未知的，需要我们进行训练后得出
- 在图像处理或者其他庞大输入的领域，可以替换一个全连接层为卷积层
- 池化层：对卷积以后的特征图像进行降维

![image-20250831210834259](E:/markdown_pic/assets/image-20250831210834259.png)



# 五、词嵌入

## 1.one-hot编码：用超级大的稀疏矩阵来表示词 （高纬度、稀疏）

![image-20250831212019002](E:/markdown_pic/assets/image-20250831212019002.png)

## 2.词嵌入 word embedding：用以形成具有相关性的合适维度的词汇表示向量



### 1）特征可表示性：

向量的某个位置处可以理解为一个特征，但这是我们人类无法理解的特征

![image-20250831212027775](E:/markdown_pic/assets/image-20250831212027775.png)

### 2）向量之间的相关性的表示：点积/余弦相似度

在这层意义上，将自然语言之间的联系转为了可以用数学公式计算出来的方式

![image-20250831214355047](E:/markdown_pic/assets/image-20250831214355047.png)

### 3）嵌入矩阵：

- 每一列就为一个词的嵌入向量，每一行是某个人类无法理解的特征

- 嵌入矩阵并非是我们直接赋值得到，而是通过深度学习的方法训练出来的
- 潜空间：词向量的维度特别高，其所在的空间

![image-20250831213306112](E:/markdown_pic/assets/image-20250831213306112.png)



# 六 RNN：使网络捕捉上下文依赖和时序信息

## 1. 使得每个输出具有时序信息

方法为：每一次在输入上面都加入一个标记

![image-20250831220630718](E:/markdown_pic/assets/image-20250831220630718.png)

 ## 2.捕捉上下文依赖：隐藏状态hi

以下图为例，对于第一步x1,不直接输出其最终的输出y1,而是先输出一个h1,这个h1作为第二步与x2的共同输入

![image-20250831222546454](E:/markdown_pic/assets/image-20250831222546454.png)

这样的话 x1的信息可以传给x2,x2可以传给x3，一直到最后的xn

![image-20250831222954008](E:/markdown_pic/assets/image-20250831222954008.png)

## 3. 结构：共有三类w矩阵

- Wxh: 计算词向量到h
- Whh:计算h与x到h
- Why:计算h到y

![image-20250831223154448](E:/markdown_pic/assets/image-20250831223154448.png)

在矩阵的角度进行运算:

![image-20250831223539442](E:/markdown_pic/assets/image-20250831223539442.png)

## 4.RNN的缺陷

- 无法捕捉长期依赖：如果句子太长，根据加权计算的过程，会导致两个相隔很远的时间步之间的联系丢失
- 无法进行并行计算：根据顺序进行


# 七 transformer
*摒弃所有的顺序时间步，将所有信息尽收眼底*

## 1.全局视角下：

#### 借助自注意力层机制，将最初输入的词向量变为了一组新的词向量，新词向量中每一个都包含了位置信息+上下文信息

![image-20250831225739974](E:/markdown_pic/assets/image-20250831225739974.png)



## 2.注意力机制：忽略时序，仍旧有上下文依赖的信息



### 1）.本质：

### 将一个查询（query）和一系列“键值对”（key-value）映射到一个输出的过程

![image-20250826224338821](E:\markdown_pic\assets\image-20250826224338821-1756306392414-1.png)

 ##  2）.最早的注意力机制



![image-20250826224550244](E:\markdown_pic\assets\image-20250826224550244-1756306392414-3.png)

### 3.）自注意力机制

![image-20250831231542359](E:/markdown_pic/assets/image-20250831231542359.png)

就是Q矩阵与K矩阵相乘，经过缩放，再进行softmax处理，最后与V相乘

![image-20250826224956750](E:\markdown_pic\assets\image-20250826224956750-1756306392414-2.png)

### 计算流程：以模型输入维度（词嵌入向量的长度）＝键维度＝值维度 为例

![image-20250826230254851](E:\markdown_pic\assets\image-20250826230254851-1756306392414-4.png)

### step1：初始化权重，生成Q K V 三个矩阵

- 设定一个权重矩阵（大小为 dmodel x dk） 通过反向传播进行学习

![image-20250826230308391](E:\markdown_pic\assets\image-20250826230308391-1756306392414-5.png)                                                                 

- 通过矩阵乘法计算获得QKV

  ![image-20250826230428675](E:\markdown_pic\assets\image-20250826230428675-1756306392414-6.png)

### step2 :计算注意力分数（attention score）：通过计算 Q矩阵 和 K矩阵的转置得到，表示查询向量与键向量的相似程度



![image-20250826231302681](E:\markdown_pic\assets\image-20250826231302681-1756306392414-7.png)

### step3【不改变矩阵】：缩放（scaling）：对点积结果进行缩放（按照k的维度的平方根进行）

![image-20250826231536241](E:\markdown_pic\assets\image-20250826231536241-1756306392414-8.png)

### step【不改变矩阵】： softmax归一化：对缩放后的分数使用softmax，得到注意力权重（每行和为1）

- 总的式子：

![image-20250826231834906](E:\markdown_pic\assets\image-20250826231834906-1756306392414-9.png)

- 分开计算：

![image-20250826231854577](E:\markdown_pic\assets\image-20250826231854577-1756306392414-10.png)

### step5:加权求和：将 最终得到的注意力权重矩阵A 与 V 相乘，得到self-attention层的最终输出

![image-20250826232340638](E:\markdown_pic\assets\image-20250826232340638-1756306392414-11.png)

- A矩阵中每一行都代表了输入序列X的每一个词，对包括其在内所有词的关注程度，由于一开始有两个输入向量：
  - x1 [1010] 
  - x2 [0202]

最后得到的A矩阵的格式为2x2，所以刚好能够反映自身与另一个输入序列的关注程度，比如对于x1  其注意力序列是[0.076 0.924]，表示在更新X1的时候，模型对X2的关注程度要远远高于X1自身。

- 也就是说每个向量都融合了输入序列x中所有词的信息



## 4）缩放因子：控制方差，使softmax保持在一个健康敏感的区域

![image-20250826233244419](E:\markdown_pic\assets\image-20250826233244419-1756306392414-12.png)

![image-20250826233447213](E:\markdown_pic\assets\image-20250826233447213-1756306392414-14.png)

所以当DK维度变大时候，点积的波动也会显著变大，为了控制Q与K的点积的波动，使其恒定为1,与输入维度无关

![image-20250826233629415](E:\markdown_pic\assets\image-20250826233629415-1756306392414-13.png)

#### 与输入的梯度无关：控制训练时候的梯度稳定性，这和softmax有关，如果qk的点积太大，也就是softmax的输入很大的话，梯度会很小，影响模型训练

![image-20250826233819361](E:\markdown_pic\assets\image-20250826233819361-1756306392414-15.png)

![image-20250826234147950](E:\markdown_pic\assets\image-20250826234147950-1756306392414-18.png)

### 单头注意力机制的问题：

1. 只用了一组QKV，所以没有办法捕获多种关系，而是依赖于捕获特定的关系（例如语法上的关系，而忽视掉语音的关系）
2. 表达能力有限 仅仅一个子空间



## 5）多头注意力机制：

![image-20250831231744465](E:/markdown_pic/assets/image-20250831231744465.png)

根据多个W矩阵将QKV计算出多组到不同的头中，将头拼接后进行线性变换



简单来说：就是计算多组qkv，允许模型同时关注不同位置、不同表示子空间的信息



- 新参数：h 表示注意力头的数量 将dmodel 的维度拆分给了h个独立的头

![image-20250826235921079](E:\markdown_pic\assets\image-20250826235921079-1756306392414-16.png)

### 独立的线性变换——独立的注意力计算——独不同的“视角/子空间”

#### 然后将每个头再进行contact操作，会保留每个头的信息



### 计算流程：

![image-20250827000433306](E:\markdown_pic\assets\image-20250827000433306-1756306392414-17.png)

序列长度n 为2 ;两个头；模型的维度为4 则k的维度就是dmodel/h=2,原因是它们后续需要拼接起来，保证其长度还是为2



### step1:为每个注意力头生成Q K V矩阵 ，需要将dmodel=4的矩阵（也就是序列长为4的输入）投影到维度为DK也就是2的矩阵

![image-20250827215636178](E:\markdown_pic\assets\image-20250827215636178-1756306392414-19.png)

### step2:每个头计算注意力分数+缩放+softmax+加权求和 得到注意力输出

![image-20250827221147880](E:\markdown_pic\assets\image-20250827221147880-1756306392414-20.png)

### step3:拼接（concatenate）操作：将所有注意力头的输出向量Z1 Z2按照列进行拼接，得到一个维度为nx(h·dk)的矩阵 也就是nxdmodel

![image-20250827222445718](E:\markdown_pic\assets\image-20250827222445718-1756306392415-21.png)

### step4:线性投影：将拼接后的矩阵投影回dmodel

![image-20250827222907579](E:\markdown_pic\assets\image-20250827222907579-1756306392415-22.png)



## 自注意力机制在transformer构架中的应用

1. 编码器encoder中的自注意力层
2. 解码器中的自注意力层：会使用掩码屏蔽未来的信息
3. 编码器-解码器注意力层：Q来自前一层解码器输出 K和V来自编码器的最终输出



## 4.transformer的构架

### 编码器+解码器

![image-20250831230106733](E:/markdown_pic/assets/image-20250831230106733-1756652554549-1.png)

### step1:将输入的内容通过词嵌入的方式转为词向量矩阵

![image-20250831230254620](E:/markdown_pic/assets/image-20250831230254620.png)

### step2: 在词嵌入矩阵上加入位置信息

加入一个形状一样的矩阵进行加法

![image-20250831230622880](E:/markdown_pic/assets/image-20250831230622880.png)

### step3:第一次经过多头注意力处理——获得上下文信息

可以看到输出的矩阵维度和输入矩阵维度没有变化，**但是具有了上下文信息**

![image-20250831230727276](E:/markdown_pic/assets/image-20250831230727276.png)

### step4 添加残差网络和归一化处理——解决梯度消失，稳定分布

![image-20250831231300791](E:/markdown_pic/assets/image-20250831231300791.png)



### step 5 送入一个全连接神经网络再进行残差和归一化处理

![image-20250831232233859](E:/markdown_pic/assets/image-20250831232233859.png)

### step 6 上一步获得的结果送入到解码器的一个多头注意力机制中，但只送入两个输入作为K V矩阵

![image-20250831232425399](E:/markdown_pic/assets/image-20250831232425399.png)



### step7  来到解码器部分，同样经过词嵌入、引入位置编码，经过多头注意力处理和残差归一化处理



### ✔有一点小不同：掩码

这里有一个再多头注意力的地方有一个掩码的过程

**目的**：使得翻译是一个词一个词进行  ，输出第一个词的时候是看不到后面的词的





![image-20250831233114618](E:/markdown_pic/assets/image-20250831233114618.png)

### Step 8 将结果 Q 送入到多头注意力机制的一个输入中 

![image-20250831233203770](E:/markdown_pic/assets/image-20250831233203770.png)

## Step 9 现在 Q K V 都全了，进行多头注意力处理 在进行残差和归一化处理 在放到一个全连接层，再进行残差和归一化处理

![image-20250831233405529](E:/markdown_pic/assets/image-20250831233405529.png)

## Step 10 再进行一层线性变换的神经网络，将输出投射到词表向量中



![image-20250831233613751](E:/markdown_pic/assets/image-20250831233613751.png)

## Step  11 经过softmax层转化为概率

概率最高的就是下一个词

![image-20250831233638690](E:/markdown_pic/assets/image-20250831233638690.png)





# 相关的术语：

- 函数 function
- 符号主义 symbolism：找精确函数解释一切原理
- 联结主义 connectionism  :找逼近的函数 不断调整
- 模型 model ：联结主义找的复杂的函数
  - 大模型 large model ：参数量很大的模型
    - 大语言模型LLM：用于自然语言处理的大模型

- 权重 weight ：模型中的参数
- 训练 training ：调整参数的过程
- 预训练 pretraining:事先训练好的模型
- 微调 fine-tuning：基于预训练模型再次进行训练，使得模型学会具体的任务
- 推理 inference：将模型的参数固定，进行有输入的输出
- GPT generative pre-trained transformer ：生成式预训练 transformer
- 闭源模型 closed-source model ：只提供服务（推理），不提供原代码和权重
- 开放权重 open-weight model：开放权重
- 完全开源 fully open-source model：开放了源代码、结构和权重
-  生成式AI 
- TOKEN
- 上下文 context ：所有给到大模型的信息
- 提示词 prompt：本质还是上下文
- 随机性 randomness
  - 温度 temperature：控制输出的随机性的参数
  - top-k: 控制随机范围在最高的K个词中取得
- 幻觉hallucination：在语言上说得通，在事实上说不通
- 联网 browsing ：回答前进行搜索，避免幻觉
- 检索增强生成 RAG (retrieval-Augmented Generation):在私有的数据库中查找答案，与 联网的思路一样
  - 知识库 KB (knowledge base)：私有的数据库
  - 向量数据库 vector database：知识会以向量的形式进行存储
- 词嵌入embedding：将词转为词向量的形式
- 向量检索 vector search:对比词向量之间的相似度，以在知识库中找到相关问题的答案的方式
- PGC professionally generated content ：由专家/团队生成的内容
  - UGC user-------:用户创造
  - AIGC:AI创造
  - AGI 通用人工智能
- 多模态 multimodal ：处理多种数据模式的能力
- 工作流 workflow：多个步骤编排在一个流程中
  - 扣子 coze：在界面上进行工作流程的编排
- Langchain :以代码的方式编排工作流
-  智能体 agent ：封装了大模型和工具集，自动完成某一类复杂工作流程
  - 多智能体 multi-agent ：多个智能体封装在一起进行更复杂的工作
- MCP model context protocol：AI与外界交互的协议
  - A2A (agent-to-agent protocol) ：用于智能体与智能体交互的协议



其他卷的方向：

- 模型压缩 model compression  ：让模型更小
- 量化 quantization ：使模型中参数的浮点数精度更小
- 蒸馏 distillation：用一个大模型teacher来指导一个小模型student进行学习
- 剪枝 pruning ：删除不重要的神经元，使得模型稀疏，提高速度
- LoRA low-rank adaptation:更低成本改善微调
- 思维链 chain-of-thought ：从推理能力增强模型推理的方式
- RLHF:人类反馈强化学习
- 套壳：封装现有大模型并提供服务